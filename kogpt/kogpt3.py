import numpy as np
import pandas as pd
from tqdm import tqdm
import re
import torch
from torch.utils.data import DataLoader, Dataset
from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel

'''
urllib.request.urlretrieve(
    "https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv",
    filename="ChatBotData.csv",
)'''
Chatbot_Data = pd.read_csv('C:/MyProject/kogpt/kogpt/data/ChatBotData.csv')

# Test ìš©ìœ¼ë¡œ 300ê°œ ë°ì´í„°ë§Œ ì²˜ë¦¬í•œë‹¤.
# Chatbot_Data = Chatbot_Data[:300]
# Chatbot_Data.head()
USE_CUDA = torch.cuda.is_available()
device = torch.device('cuda:0')

Q_TKN = "<usr>"
A_TKN = "<sys>"
BOS = '</s>'
EOS = '</s>'
MASK = '<unused0>'
SENT = '<unused1>'
PAD = '<pad>'

# í—ˆê¹…í˜ì´ìŠ¤ transformers ì— ë“±ë¡ëœ ì‚¬ì „ í•™ìŠµëœ koGTP2 í† í¬ë‚˜ì´ì €ë¥¼ ê°€ì ¸ì˜¨ë‹¤.
# koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2", bos_token=BOS, eos_token=EOS, unk_token="<unk>", pad_token=PAD, mask_token=MASK,)

tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2",
         bos_token=BOS, eos_token=EOS, unk_token='<unk>', pad_token=PAD, mask_token=MASK) 
model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2').to(device)

# PATH = 'C:/MyProject/kogpt/kogpt/save/chatbot_v100.pt'
# model = torch.load(PATH)

# ì±—ë´‡ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ë¥¼ ë§Œë“ ë‹¤.
class ChatbotDataset(Dataset):
    def __init__(self, chats, max_len=40):  # ë°ì´í„°ì…‹ì˜ ì „ì²˜ë¦¬ë¥¼ í•´ì£¼ëŠ” ë¶€ë¶„
        self._data = chats
        self.max_len = max_len
        self.q_token = Q_TKN
        self.a_token = A_TKN
        self.sent_token = SENT
        self.eos = EOS
        self.mask = MASK
        self.tokenizer = tokenizer

    def __len__(self):  # chatbotdata ì˜ ê¸¸ì´ë¥¼ ë¦¬í„´í•œë‹¤.
        return len(self._data)

    def __getitem__(self, idx):  # ë¡œë“œí•œ ì±—ë´‡ ë°ì´í„°ë¥¼ ì°¨ë¡€ì°¨ë¡€ DataLoaderë¡œ ë„˜ê²¨ì£¼ëŠ” ë©”ì„œë“œ
        turn = self._data.iloc[idx]
        q = turn["Q"]  # ì§ˆë¬¸ì„ ê°€ì ¸ì˜¨ë‹¤.
        q = re.sub(r"([?.!,])", r" ", q)  # êµ¬ë‘£ì ë“¤ì„ ì œê±°í•œë‹¤.

        a = turn["A"]  # ë‹µë³€ì„ ê°€ì ¸ì˜¨ë‹¤.
        a = re.sub(r"([?.!,])", r" ", a)  # êµ¬ë‘£ì ë“¤ì„ ì œê±°í•œë‹¤.

        q_toked = self.tokenizer.tokenize(self.q_token + q + self.sent_token)
        q_len = len(q_toked)

        a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos)
        a_len = len(a_toked)

        #ì§ˆë¬¸ì˜ ê¸¸ì´ê°€ ìµœëŒ€ê¸¸ì´ë³´ë‹¤ í¬ë©´
        if q_len > self.max_len:
            a_len = self.max_len - q_len        #ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ - ì§ˆë¬¸ê¸¸ì´
            if a_len <= 0:       #ì§ˆë¬¸ì˜ ê¸¸ì´ê°€ ë„ˆë¬´ ê¸¸ì–´ ì§ˆë¬¸ë§Œìœ¼ë¡œ ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼ í•œë‹¤ë©´
                q_toked = q_toked[-(int(self.max_len / 2)) :]   #ì§ˆë¬¸ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ì˜ ë°˜ìœ¼ë¡œ 
                q_len = len(q_toked)
                a_len = self.max_len - q_len              #ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ - ì§ˆë¬¸ê¸¸ì´
            a_toked = a_toked[:a_len]
            a_len = len(a_toked)

        #ì§ˆë¬¸ì˜ ê¸¸ì´ + ë‹µë³€ì˜ ê¸¸ì´ê°€ ìµœëŒ€ê¸¸ì´ë³´ë‹¤ í¬ë©´
        if q_len + a_len > self.max_len:
            a_len = self.max_len - q_len        #ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ - ì§ˆë¬¸ê¸¸ì´
            if a_len <= 0:       #ì§ˆë¬¸ì˜ ê¸¸ì´ê°€ ë„ˆë¬´ ê¸¸ì–´ ì§ˆë¬¸ë§Œìœ¼ë¡œ ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼ í•œë‹¤ë©´
                q_toked = q_toked[-(int(self.max_len / 2)) :]   #ì§ˆë¬¸ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ì˜ ë°˜ìœ¼ë¡œ 
                q_len = len(q_toked)
                a_len = self.max_len - q_len              #ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ - ì§ˆë¬¸ê¸¸ì´
            a_toked = a_toked[:a_len]
            a_len = len(a_toked)

        # ë‹µë³€ labels = [mask, mask, ...., mask, ..., <bos>,..ë‹µë³€.. <eos>, <pad>....]
        labels = [self.mask,] * q_len + a_toked[1:]

        # mask = ì§ˆë¬¸ê¸¸ì´ 0 + ë‹µë³€ê¸¸ì´ 1 + ë‚˜ë¨¸ì§€ 0
        mask = [0] * q_len + [1] * a_len + [0] * (self.max_len - q_len - a_len)
        # ë‹µë³€ labelsì„ index ë¡œ ë§Œë“ ë‹¤.
        labels_ids = self.tokenizer.convert_tokens_to_ids(labels)
        # ìµœëŒ€ê¸¸ì´ë§Œí¼ PADDING
        while len(labels_ids) < self.max_len:
            labels_ids += [self.tokenizer.pad_token_id]

        # ì§ˆë¬¸ + ë‹µë³€ì„ index ë¡œ ë§Œë“ ë‹¤.    
        token_ids = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked)
        # ìµœëŒ€ê¸¸ì´ë§Œí¼ PADDING
        while len(token_ids) < self.max_len:
            token_ids += [self.tokenizer.pad_token_id]

        #ì§ˆë¬¸+ë‹µë³€, ë§ˆìŠ¤í¬, ë‹µë³€
        return (token_ids, np.array(mask), labels_ids)

def collate_batch(batch):
    data = [item[0] for item in batch]
    mask = [item[1] for item in batch]
    label = [item[2] for item in batch]
    return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)

train_set = ChatbotDataset(Chatbot_Data, max_len=40)

#ìœˆë„ìš° í™˜ê²½ì—ì„œ num_workers ëŠ” ë¬´ì¡°ê±´ 0ìœ¼ë¡œ ì§€ì •, ë¦¬ëˆ…ìŠ¤ì—ì„œëŠ” 2
train_dataloader = DataLoader(train_set, batch_size=32, num_workers=0, shuffle=True, collate_fn=collate_batch,)

print("start")
for batch_idx, samples in enumerate(tqdm(train_dataloader)):
    token_ids, mask, label = samples
    '''print("token_ids ====> ", token_ids)
    print("mask =====> ", mask)
    print("label =====> ", label)'''
print("end")

tokenizer.tokenize("ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o")

'''text = 'ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ”'
input_ids = tokenizer.encode(text)
gen_ids = model.generate(torch.tensor([input_ids]),
                           max_length=128,
                           repetition_penalty=2.0,
                           pad_token_id=tokenizer.pad_token_id,
                           eos_token_id=tokenizer.eos_token_id,
                           bos_token_id=tokenizer.bos_token_id,
                           use_cache=True)
generated = tokenizer.decode(gen_ids[0,:].tolist())
print(generated)'''


#model.to(device)
model.train()

learning_rate = 5e-5
criterion = torch.nn.CrossEntropyLoss(reduction="none")
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

epoch = 80
Sneg = -1e18

    
print ("start")
for epoch in range(epoch):
    model.train()
    log_interval = 200
    for batch_idx, samples in enumerate(tqdm(train_dataloader)):
        optimizer.zero_grad()
        token_ids, mask, label = samples
        token_ids = token_ids.long().to(device)
        mask = mask.long().to(device)
        label = label.long().to(device)
        out = model(token_ids)
        out = out.logits      #Returns a new tensor with the logit of the elements of input
        mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2)
        mask_out = torch.where(mask_3d == 1, out, Sneg * torch.ones_like(out))
        loss = criterion(mask_out.transpose(2, 1), label)
        #loss.mean().backward()
        # í‰ê·  loss ë§Œë“¤ê¸° avg_loss[0] / avg_loss[1] <- loss ì •ê·œí™”
        avg_loss = loss.sum() / mask.sum()
        avg_loss.backward()
        # í•™ìŠµ ë
        optimizer.step()
        if batch_idx % log_interval == 0:
            print("epoch {} batch_idx {} loss {}".format(epoch+1, batch_idx+1, avg_loss))
                 
print ("end")

#ëª¨ë¸ ì €ì¥
PATH = 'C:/MyProject/kogpt/kogpt/save/chatbot_v80.pt'
torch.save(model, PATH)


